{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0)\n",
    "\n",
    "- Dynamic Programming Algorithms\n",
    "\n",
    "    - Policy iteration (includes policy evaluation and policy improvement)\n",
    "    - Value iteration\n",
    "\n",
    "- Temporal Different Algorithms\n",
    "    - Sarsa\n",
    "    - Q-learning\n",
    "    \n",
    "- Optional algorithms (need not submit)\n",
    "    - Double q-learning\n",
    "    - Expected Sarsa\n",
    "   \n",
    "\n",
    "The sub-routines for the algorithms are in `algorithms.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hocke\\anaconda3\\envs\\hw6\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from lake_envs import *\n",
    "from algorithms import *\n",
    "np.set_printoptions(precision=3)\n",
    "# Make the two versions of the environment\n",
    "env_d = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "env_s = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "RENDER_ENV = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "## Policy Iteration on Deterministic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "V_pi, p_pi = policy_iteration(env_d.P, env_d.nS, env_d.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env_d, p_pi, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Value Iteration on Deterministic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "V_vi, p_vi = value_iteration(env_d.P, env_d.nS, env_d.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env_d, p_vi, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration  on Stochastic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "V_pi, p_pi = policy_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env_s, p_pi, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration on Stochastic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23444/2570182140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mV_vi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_vi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrender_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_vi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_rendering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRENDER_ENV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\hocke\\OneDrive\\Desktop\\EricZacharia\\03-Education\\06-Autumn2021\\ADA\\Homework\\homework6\\hw6_code\\algorithms.py\u001b[0m in \u001b[0;36mrender_single\u001b[1;34m(env, policy, max_steps, show_rendering)\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshow_rendering\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "V_vi, p_vi = value_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env_s, p_vi, 100, show_rendering=RENDER_ENV) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Dynamic Programming Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate policy iteration on stochastic FrozenLake.  Running the same policy over multiple episodes will result in different outcomes (final reward) due to stochastic transitions in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration on Stochastic FrozenLake:\n",
      "> Average reward over 1000 episodes:\t\t\t 0.722\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 94%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.722"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Policy Iteration on Stochastic FrozenLake:\")\n",
    "V_pi, p_pi = policy_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)\n",
    "evaluate(env_s, p_pi, max_steps=100, max_episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate value iteration on stochastic FrozenLake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Iteration on Stochastic FrozenLake:\n",
      "> Average reward over 1000 episodes:\t\t\t 0.699\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 93%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.699"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nValue Iteration on Stochastic FrozenLake:\")\n",
    "V_vi, p_vi = value_iteration(env_s.P, env_s.nS, env_s.nA, gamma=0.9, tol=1e-3)\n",
    "evaluate(env_s, p_vi, max_steps=100, max_episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference\n",
    "## Sarsa on Deterministic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Sarsa on the deterministic environment\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Sarsa on the deterministic environment\\n\" + \"-\"*25)\n",
    "\n",
    "Q_ql, p_ql = sarsa(env_d, env_d.nS, env_d.nA, gamma=0.9, epsilon=0.6,\n",
    "                   alpha=0.05, max_steps=100, max_episodes=100000)\n",
    "display(p_ql)\n",
    "render_single(env_d, p_ql, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa on Stochastic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Q-learning on the stochastic environment\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 3, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Sarsa on the stochastic environment\\n\" + \"-\"*25)\n",
    "\n",
    "Q_ql, p_ql  = sarsa(env_s, env_s.nS, env_s.nA, gamma=0.9, epsilon = 0.6,\n",
    "                               alpha = 0.05, max_steps = 100, max_episodes=100000)\n",
    "display(p_ql)\n",
    "render_single(env_s, p_ql, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning on Deterministic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Q-learning on the deterministic environment\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Q-learning on the deterministic environment\\n\" + \"-\"*25)\n",
    "\n",
    "Q_ql, p_ql  = q_learning(env_d, env_d.nS, env_d.nA, gamma=0.9, epsilon = 0.6,\n",
    "                         alpha=0.05, max_steps=100, max_episodes=200000)\n",
    "display(p_ql)\n",
    "render_single(env_d, p_ql, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning on Stochastic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Q-learning on the stochastic environment\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 2, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Q-learning on the stochastic environment\\n\" + \"-\"*25)\n",
    "\n",
    "Q_ql, p_ql  = q_learning(env_s, env_s.nS, env_s.nA, gamma=0.9, epsilon = 0.6,\n",
    "                         alpha=0.05, max_steps=100, max_episodes=200000)\n",
    "display(p_ql)\n",
    "render_single(env_s, p_ql, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Temporal Difference Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sarsa on Stochastic FrozenLake:\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1183\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1183"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nSarsa on Stochastic FrozenLake:\")\n",
    "Q_ql, p_ql  = sarsa(env_s, env_s.nS, env_s.nA, gamma=0.9, epsilon = 0.6,\n",
    "                               alpha = 0.05, max_steps = 100, max_episodes=100000)\n",
    "evaluate(env_s, p_ql, max_steps=200, max_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Learning on Stochastic FrozenLake:\n",
      "> Average reward over 10000 episodes:\t\t\t 0.418\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.418"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nQ-Learning on Stochastic FrozenLake:\")\n",
    "Q_ql, p_ql  = q_learning(env_s, env_s.nS, env_s.nA, gamma=0.9, epsilon = 0.6,\n",
    "                         alpha=0.05, max_steps=100, max_episodes=200000)\n",
    "evaluate(env_s, p_ql, max_steps=200, max_episodes=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-Learning on Deterministic Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Double Q-learning on the deterministic environment\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Double Q-learning on the deterministic environment\\n\" + \"-\"*25)\n",
    "\n",
    "Q_ql, Q_q2, p_ql = double_q_learning(env_d, env_d.nS, env_d.nA, gamma=0.9, epsilon=0.6,\n",
    "                                     alpha=0.05, max_steps=100, max_episodes=200000)\n",
    "display(p_ql)\n",
    "render_single(env_d, p_ql, 100, show_rendering=RENDER_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Double Q-Learning on Stochastic FrozenLake:\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6281\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6281"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nDouble Q-Learning on Stochastic FrozenLake:\")\n",
    "Q_ql, Q_q2, p_ql = double_q_learning(env_s, env_s.nS, env_s.nA, gamma=0.9, epsilon=0.6,\n",
    "                                     alpha=0.05, max_steps=100, max_episodes=200000)\n",
    "evaluate(env_s, p_ql, max_steps=200, max_episodes=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters(algorithm):\n",
    "    gamma = [0.8, 0.9]\n",
    "    epsilon = [0.6, 0.7, 0.8]\n",
    "    alpha = [0.05, 0.1, 0.15]\n",
    "    max_episodes = [100000, 200000]\n",
    "    max_score = 0\n",
    "    optimal_parameters = {}\n",
    "    for g in gamma:\n",
    "        for e in epsilon:\n",
    "            for a in alpha:\n",
    "                for m in max_episodes:\n",
    "                    score_sum = 0\n",
    "                    runs = 3\n",
    "                    for _ in range(runs):\n",
    "                        Q_pl, p_ql = algorithm(env_s, env_s.nS, env_s.nA, gamma=g, epsilon=e,\n",
    "                                            alpha=a, max_steps=100, max_episodes=m)\n",
    "                        score_sum += evaluate(env_s, p_ql, max_steps=200, max_episodes=10000)\n",
    "                    avg_score = score_sum / runs\n",
    "                    if avg_score > max_score:\n",
    "                        max_score = avg_score\n",
    "                        optimal_parameters['gamma'] = g\n",
    "                        optimal_parameters['epsilon'] = e\n",
    "                        optimal_parameters['alpha'] = a\n",
    "                        optimal_parameters['max_episodes'] = m\n",
    "    return optimal_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Average reward over 10000 episodes:\t\t\t 0.2142\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34068/4158520072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimal_sarsa_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msarsa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# 1 hour run time. {'gamma': 0.9, 'epsilon': 0.6, 'alpha': 0.05, 'max_episodes': 100000}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimal_sarsa_parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34068/3554886284.py\u001b[0m in \u001b[0;36moptimize_parameters\u001b[1;34m(algorithm)\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                         Q_pl, p_ql = algorithm(env_s, env_s.nS, env_s.nA, gamma=g, epsilon=e,\n\u001b[1;32m---> 16\u001b[1;33m                                             alpha=a, max_steps=100, max_episodes=m)\n\u001b[0m\u001b[0;32m     17\u001b[0m                         \u001b[0mscore_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_ql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[0mavg_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_sum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mruns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hocke\\OneDrive\\Desktop\\EricZacharia\\03-Education\\06-Autumn2021\\ADA\\Homework\\homework6\\hw6_code\\algorithms.py\u001b[0m in \u001b[0;36msarsa\u001b[1;34m(env, nS, nA, gamma, epsilon, alpha, max_steps, max_episodes)\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hw6\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hw6\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[1;34m(prob_n, np_random)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimal_sarsa_parameters = optimize_parameters(sarsa)\n",
    "# 1 hour run time. {'gamma': 0.9, 'epsilon': 0.6, 'alpha': 0.05, 'max_episodes': 100000}\n",
    "optimal_sarsa_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Average reward over 10000 episodes:\t\t\t 0.4538\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2854\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.079\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2412\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3383\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1365\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2444\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2918\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2385\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.4017\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2043\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1371\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1294\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.0716\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1009\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6102\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1975\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2126\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.317\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1018\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2364\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1595\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1667\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6407\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.4328\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1316\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.642\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.4184\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7848\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3913\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2923\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.0954\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.125\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1648\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2996\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3364\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3355\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2842\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1788\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2389\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1714\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1564\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1598\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1394\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2223\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1247\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2771\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1858\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.128\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1724\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.0382\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.073\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1501\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2478\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6775\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7728\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.5176\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7462\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6765\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6912\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.5357\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.749\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3365\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.5218\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.5203\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.5865\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2091\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2768\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1068\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6813\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2003\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3886\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.168\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.5264\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.512\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.5193\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7767\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6755\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7821\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3854\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.4204\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2426\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6228\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2336\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.0873\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.624\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.271\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.2415\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6789\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.4493\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7504\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1855\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.4629\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3928\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6231\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.684\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1425\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7529\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.4596\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1943\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6318\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 99%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.6214\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.3849\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7805\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1171\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.1716\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.5231\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n",
      "> Average reward over 10000 episodes:\t\t\t 0.7738\n",
      "> Percentage of episodes a terminal state reached:\t\t\t 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.9, 'epsilon': 0.6, 'alpha': 0.05, 'max_episodes': 200000}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_q_learning_parameters = optimize_parameters(q_learning)\n",
    "# 1 hour run time {'gamma': 0.9, 'epsilon': 0.6, 'alpha': 0.05, 'max_episodes': 200000}\n",
    "optimal_q_learning_parameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
